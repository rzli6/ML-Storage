{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>R</th>\n",
       "      <th>F</th>\n",
       "      <th>Sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CART</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        P    R    F   Sd\n",
       "CART  NaN  NaN  NaN  NaN\n",
       "SVM   NaN  NaN  NaN  NaN\n",
       "NN    NaN  NaN  NaN  NaN\n",
       "LR    NaN  NaN  NaN  NaN\n",
       "RF    NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ST3000DM001'\n",
    "data_dirs = ['../data-2015/']\n",
    "attributes = ['smart_1_raw', 'smart_4_raw', 'smart_5_raw', 'smart_7_raw', 'smart_9_raw', 'smart_12_raw', 'smart_187_raw', 'smart_193_raw', 'smart_194_raw', 'smart_197_raw', 'smart_199_raw']\n",
    "drive_models = ['ST4000DM000', 'ST3000DM001', 'Hitachi HDS5C3030ALA630', 'Hitachi HDS722020ALA330', 'Hitachi HDS5C4040ALE630', 'HGST HMS5C4040ALE640', 'HGST HMS5C4040BLE640']\n",
    "features = ['serial_number', 'date', 'capacity_bytes', 'smart_5_raw', 'smart_1_raw', 'smart_4_raw', 'smart_7_raw', 'smart_9_raw', 'smart_12_raw', 'smart_187_raw', 'smart_193_raw', 'smart_194_raw', 'smart_197_raw', 'smart_199_raw', 'smart_196_raw']\n",
    "type_dict = {feature: np.float32 for feature in features[3:]}\n",
    "\n",
    "def model_stat(model, X, y):\n",
    "    f_score = cross_val_score(model, X, y, cv=5, scoring='f1', n_jobs=-1)\n",
    "    r_score = cross_val_score(model, X, y, cv=5, scoring='recall', n_jobs=-1)\n",
    "    p_score = cross_val_score(model, X, y, cv=5, scoring='precision', n_jobs=-1)\n",
    "    return pd.Series({'P': np.mean(p_score), 'R': np.mean(r_score), 'F': np.mean(f_score), 'Sd': np.std(f_score)})\n",
    "\n",
    "statics = pd.DataFrame(index=['CART', 'SVM', 'NN', 'LR', 'RF'], columns=['P', 'R', 'F', 'Sd'])\n",
    "statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in drive_models:\n",
    "    drive_model = pd.concat([(lambda pd: pd.loc[pd['model'] == model_name][features])(pd.read_csv(os.path.join(data_dir, filename), dtype=type_dict)) for data_dir in data_dirs for filename in os.listdir(data_dir)]).sort_values(by=['serial_number', 'date'])\n",
    "    drive_model.to_csv('./preprocess/' + model_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_vars = ['smart_5_raw', 'smart_187_raw', 'smart_196_raw', 'smart_197_raw']\n",
    "overview = pd.DataFrame(index=drive_models, columns=res_vars+['Capacity (TB)', '# Drives'])\n",
    "type_dict = {key: np.float64 for key in res_vars}\n",
    "\n",
    "for model_name in drive_models:\n",
    "    drive_model = pd.read_csv('./preprocess/' + model_name + '.csv', dtype=type_dict)\n",
    "    percentage = drive_model.groupby('serial_number')[res_vars].sum().agg(lambda data: data[data>0].size/data.size)\n",
    "    percentage['Capacity (TB)'] = drive_model.iloc[0]['capacity_bytes'] // 1000**4\n",
    "    percentage['# Drives'] = drive_model['serial_number'].unique().size\n",
    "    overview.loc[model_name] = percentage\n",
    "    \n",
    "overview.to_csv('./preprocess/overview.csv')\n",
    "overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drive_model = pd.read_csv('./preprocess/' + model_name + '.csv', dtype=type_dict).iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drive_model.sort_values(by=['serial_number', 'date'])\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# week = ['W-SUN', 'W-MON', 'W-TUE', 'W-WED', 'W-THU', 'W-FRI', 'W-SAT']\n",
    "df2 = df.copy()\n",
    "df2['date'] += datetime.timedelta(days=1)\n",
    "\n",
    "day = 'W-MON'\n",
    "model_group = df.groupby(['serial_number', pd.Grouper(key='date', freq=day)])\n",
    "y = model_group['smart_5_raw'].last().to_frame().rename({'smart_5_raw': 'y'}, axis='columns')\n",
    "\n",
    "model_group2 = df2.groupby(['serial_number', pd.Grouper(key='date', freq=day)])\n",
    "input1 = model_group2[attributes].last()\n",
    "\n",
    "training_set = y.join(other=input1, how='inner')\n",
    "training_set['y'] = np.where(training_set['y'] > training_set['smart_5_raw'], 1, 0)\n",
    "input2_features = ['smart_4_raw', 'smart_5_raw', 'smart_7_raw', 'smart_9_raw', 'smart_12_raw', 'smart_187_raw', 'smart_193_raw', 'smart_197_raw', 'smart_199_raw']\n",
    "input2 = training_set.groupby(level=0)[input2_features].transform(pd.DataFrame.diff).dropna(how='any')\n",
    "training_set = training_set.join(other=input2, how='inner', rsuffix='_increase')\n",
    "    \n",
    "training_set.to_csv('./preprocess/' + model_name + '_training_set.csv')\n",
    "training_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('./preprocess/' + model_name + '_training_set.csv').drop(['serial_number', 'date'], axis='columns')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import _tree\n",
    "\n",
    "def tree_to_code(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            print(\"{}else:  # if {} > {}\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            print(\"{}return {}\".format(indent, tree_.value[node]))\n",
    "\n",
    "    recurse(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune DT\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train, y_train)\n",
    "tree_to_code(dt_model, training_set.drop(['y'], axis='columns').columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.drop(['y'], axis='columns').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down sampling\n",
    "X_health_raw = training_set.loc[training_set['y'] == 0].drop(['y'], axis='columns').values\n",
    "X_fail = training_set.loc[training_set['y'] == 1].drop(['y'], axis='columns').values\n",
    "n_clusters = X_fail.shape[0] // 5\n",
    "# kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_jobs=-1).fit(X_health_raw)\n",
    "kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=0, batch_size=50000).fit(X_health_raw)\n",
    "X_health = np.concatenate([X_health_raw[np.argsort(kmeans.transform(X_health_raw)[:, j])[::-1][:10]] for j in range(0, n_clusters)], axis=0)\n",
    "X_train_raw = np.concatenate((X_health, X_fail), axis=0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_raw)\n",
    "y_train = np.concatenate((np.zeros(X_health.shape[0]), np.ones(X_fail.shape[0])), axis=0) \n",
    "\n",
    "print(X_fail.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cart, rf (20-100 trees), svm, neural networks (3 layers 100 nodes), logistic regression\n",
    "X_transformed = PCA(n_components=0.95).fit_transform(X_train)\n",
    "X_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LR\n",
    "lr_model = LogisticRegressionCV(Cs=100, fit_intercept=True, cv=5, \n",
    "                                        dual=False, penalty='l2', scoring='f1', \n",
    "                                        solver='newton-cg',  max_iter=1000, class_weight='balanced',\n",
    "                                        n_jobs=-1, refit=True, multi_class='ovr', random_state=0, verbose=1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "statics.loc['LR'] = model_stat(lr_model, X_train, y_train)\n",
    "statics.loc['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune LR\n",
    "lr_model = LogisticRegressionCV(Cs=100, fit_intercept=True, cv=5, \n",
    "                                        dual=False, penalty='l2', scoring='f1', \n",
    "                                        solver='newton-cg',  max_iter=1000, class_weight='balanced',\n",
    "                                        n_jobs=-1, refit=True, multi_class='ovr', random_state=0, verbose=1)\n",
    "lr_model.fit(X_transformed, y_train)\n",
    "statics.loc['LR'] = model_stat(lr_model, X_transformed, y_train)\n",
    "statics.loc['LR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(y))\n",
    "# print(np.sum(y_train))\n",
    "# training_set.loc[training_set['y'] == 1].drop(['y'], axis='columns').head()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune SVM\n",
    "svm_model = SVC(C=1.05, kernel='rbf', gamma=0.05, class_weight='balanced', max_iter=-1, random_state=0)\n",
    "statics.loc['SVM'] = model_stat(svm_model, X_transformed, y_train)\n",
    "statics.loc['SVM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune RF\n",
    "rf_model = RandomForestClassifier()\n",
    "statics.loc['RF'] = model_stat(rf_model, X_transformed, y_train)\n",
    "statics.loc['RF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # rf_model.fit(X_, y_train)\n",
    "# print(rf_model.feature_importances_)\n",
    "# training_set.loc[training_set['y'] == 0].drop(['y'], axis='columns').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune DT\n",
    "dt_model = DecisionTreeClassifier()\n",
    "statics.loc['CART'] = model_stat(dt_model, X_transformed, y_train)\n",
    "statics.loc['CART']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune NN\n",
    "nn_model = MLPClassifier(hidden_layer_sizes=(100, 100, 100))\n",
    "statics.loc['NN'] = model_stat(nn_model, X_transformed, y_train)\n",
    "statics.loc['NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "statics.T.to_csv('./preprocess/' + model_name + '_finalstatics.csv')\n",
    "statics.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
