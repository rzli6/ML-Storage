{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from importlib import reload\n",
    "import find_cpt\n",
    "from rgf.sklearn import RGFClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "data_dir = \"./data-2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('./data-2015/2015-01-01.csv')\n",
    "all_features = list(test.loc[test['model'] == 'ST4000DM000'].dropna(axis=1, how='all').columns.values)\n",
    "def process_SgtA(df, name):\n",
    "    df = df.loc[df['model'] == name]\n",
    "    return df[all_features]\n",
    "type_dict = {feature: np.float32 for feature in all_features[5:]}\n",
    "SgtA = pd.concat([process_SgtA(pd.read_csv(os.path.join(data_dir, filename), dtype=type_dict), 'ST4000DM000') for filename in os.listdir(data_dir)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586\n",
      "0.019750589821368385\n"
     ]
    }
   ],
   "source": [
    "# find names of the failure disk\n",
    "fail_names = SgtA.loc[SgtA['failure'] == 1]['serial_number'].unique()\n",
    "print(fail_names.size)\n",
    "print(fail_names.size / SgtA['serial_number'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find failure disks\n",
    "failure_disk_group = SgtA.loc[SgtA['serial_number'].isin(fail_names)].sort_values('date', ascending=True).groupby('serial_number')\n",
    "\n",
    "def get_cpt(data):\n",
    "    changepoint = find_cpt.cpt(data=data, type='normal-mean').find_changepoint()\n",
    "    if changepoint > 0:\n",
    "        return data.size - changepoint\n",
    "    return changepoint\n",
    "\n",
    "functions_group = {n: get_cpt for n in all_features[5:]}\n",
    "all_cpt_series = failure_disk_group.agg(functions_group)\n",
    "# print(all_cpt_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent(data):\n",
    "    return data[(data>0) & (data <=100)].dropna().size/data.dropna().size\n",
    "def get_median(data):\n",
    "    return data[data>0].dropna().median()\n",
    "def get_mean(data):\n",
    "    return data[data>0].dropna().mean()\n",
    "summarize = all_cpt_series.agg([get_percent, get_median, get_mean])\n",
    "# print(summarize)\n",
    "summarize.to_csv('./preprocess/summarize.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['smart_7_raw', 'smart_190_normalized', 'smart_194_normalized',\n",
      "       'smart_194_raw', 'smart_190_raw', 'smart_193_raw', 'smart_9_raw',\n",
      "       'smart_242_raw', 'smart_241_raw', 'smart_9_normalized',\n",
      "       'smart_7_normalized', 'smart_198_raw', 'smart_197_raw', 'smart_240_raw',\n",
      "       'smart_187_raw', 'smart_187_normalized', 'smart_193_normalized',\n",
      "       'smart_4_raw', 'smart_12_raw', 'smart_1_normalized', 'smart_1_raw',\n",
      "       'smart_3_normalized', 'smart_5_raw', 'smart_189_raw',\n",
      "       'smart_189_normalized', 'smart_5_normalized', 'smart_192_raw',\n",
      "       'smart_198_normalized', 'smart_197_normalized', 'smart_188_raw',\n",
      "       'smart_183_normalized', 'smart_183_raw', 'smart_184_normalized',\n",
      "       'smart_184_raw', 'smart_199_raw'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "selected_features = summarize.loc['get_percent'].T.sort_values(ascending=False)\n",
    "selected_features = selected_features[selected_features>0.01].index\n",
    "# irrelevent_features = ['smart_9_raw', 'smart_9_normalized', 'smart_4_raw', 'smart_4_normalized', 'smart_12_raw', 'smart_12_normalized']\n",
    "# selected_features = [i for i in selected_features if i not in irrelevent_features]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smart_3_normalized', 'smart_192_raw', 'smart_183_normalized', 'smart_183_raw', 'smart_199_raw']\n"
     ]
    }
   ],
   "source": [
    "Sgt_features = ['serial_number', 'date', 'smart_1_normalized', 'smart_1_raw', 'smart_5_normalized', 'smart_5_raw', 'smart_7_normalized', 'smart_7_raw',\n",
    "    'smart_184_normalized', 'smart_184_raw', 'smart_187_normalized', 'smart_187_raw', 'smart_188_raw', 'smart_189_normalized', 'smart_189_raw', \n",
    "    'smart_190_normalized', 'smart_190_raw', 'smart_193_normalized', 'smart_193_raw', 'smart_194_normalized', 'smart_194_raw', 'smart_197_normalized', \n",
    "    'smart_197_raw', 'smart_198_normalized', 'smart_198_raw', 'smart_240_raw', 'smart_241_raw', 'smart_242_raw', 'failure']\n",
    "# new_features = [i for i in selected_features if i not in Sgt_features]\n",
    "new_features = [i for i in selected_features if i not in Sgt_features]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_3_normalized      0.240614\n",
      "smart_192_raw           0.081911\n",
      "smart_183_normalized    0.044369\n",
      "smart_183_raw           0.044369\n",
      "smart_199_raw           0.018771\n",
      "Name: get_percent, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(summarize[new_features].loc['get_percent'])\n",
    "# 3: Spin-Up Time (NA)\n",
    "# 4: Start/Stop Count (not in)\n",
    "# 9: Power-On Hours (not in)\n",
    "# 12: Power Cycle Count (not in)\n",
    "# 183: SATA Downshift Error Count or Runtime Bad Block (0.5%)\n",
    "# 192: Power-off Retract Count, Emergency Retract Cycle Count (not in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ray\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: pd.ewm_mean is deprecated for ndarrays and will be removed in a future version\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# compact info.\n",
    "def get_cmpt_info(data):\n",
    "    return pd.ewma(data.values, span=np.round(summarize.loc['get_median', data.name]))[-1]\n",
    "functions_group1 = {n: get_cmpt_info for n in selected_features}\n",
    "# print(functions_group1)\n",
    "compacted_info = SgtA.groupby('serial_number', as_index=False).agg(functions_group1)\n",
    "compacted_info['failure'] = compacted_info.apply(lambda row: 1 if row['serial_number'] in fail_names else 0, axis=1)\n",
    "# print(compacted_info)\n",
    "compacted_info.to_csv('./preprocess/compacted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kmeans\n",
    "# prepare set\n",
    "X_health = compacted_info.loc[compacted_info['failure'] == 1].drop(['serial_number', 'failure'], axis=1).values\n",
    "kmeans = KMeans(n_clusters=150, random_state=0, n_jobs=-1).fit(X_health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_health_transformed = []\n",
    "for j in range(0, 150):\n",
    "    d = kmeans.transform(X_health)[:, j]\n",
    "    ind = np.argsort(d)[::-1][:10]\n",
    "    X_health_transformed[0:0] = list(X_health[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_failed = list(compacted_info.loc[compacted_info['failure'] == 1].drop(['serial_number', 'failure'], axis=1).values)\n",
    "X = X_health_transformed + X_failed\n",
    "y = np.concatenate((np.zeros(len(X_health_transformed)), np.ones(len(X_failed))), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_stat(model, X, y):\n",
    "    f_score = np.average(cross_val_score(model, X, y, cv=5, scoring='f1', n_jobs=-1))\n",
    "    r_score = np.average(cross_val_score(model, X, y, cv=5, scoring='recall', n_jobs=-1))\n",
    "    p_score = np.average(cross_val_score(model, X, y, cv=5, scoring='precision', n_jobs=-1))\n",
    "    print(f_score)\n",
    "    print(r_score)\n",
    "    print(p_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    3.8s remaining:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    5.2s finished\n",
      "C:\\Users\\Ray\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:461: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\Ray\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:312: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "C:\\Users\\Ray\\Anaconda3\\lib\\site-packages\\scipy\\optimize\\linesearch.py:421: LineSearchWarning: Rounding errors prevent the line search from converging\n",
      "  warn(msg, LineSearchWarning)\n",
      "C:\\Users\\Ray\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\optimize.py:195: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6253713700459477\n",
      "0.7967260611328408\n",
      "0.5160479460623386\n"
     ]
    }
   ],
   "source": [
    "# tune LR\n",
    "lr_model = LogisticRegressionCV(Cs=100, fit_intercept=True, cv=5, \n",
    "                                        dual=False, penalty='l2', scoring='f1', \n",
    "                                        solver='newton-cg',  max_iter=1000, class_weight='balanced',\n",
    "                                        n_jobs=-1, refit=True, multi_class='ovr', random_state=0, verbose=1)\n",
    "lr_model.fit(X, y)\n",
    "model_stat(lr_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.3s remaining:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9116397726629257\n",
      "0.8690424453136318\n",
      "0.969636073814604\n",
      "{'bootstrap': False, 'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 12, 'max_features': 'auto', 'max_leaf_nodes': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0, 'n_estimators': 5, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# tune RF\n",
    "n_estimators = [5] # tuned\n",
    "max_features = ['auto'] # tuned\n",
    "criterion = ['entropy']  # tuned\n",
    "max_depth = [12] # tuned\n",
    "min_samples_split = [2]\n",
    "min_samples_leaf = [1]\n",
    "min_weight_fraction_leaf = [0]\n",
    "max_leaf_nodes = [None]\n",
    "\n",
    "bootstrap = [False]\n",
    "random_state = [0]\n",
    "class_weight = ['balanced']\n",
    "search_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'criterion': criterion,\n",
    "    'max_features': max_features,\n",
    "    'max_depth': max_depth,\n",
    "    'min_samples_split': min_samples_split,\n",
    "    'min_samples_leaf': min_samples_leaf,\n",
    "    'min_weight_fraction_leaf': min_weight_fraction_leaf,\n",
    "    'max_leaf_nodes': max_leaf_nodes,\n",
    "    'bootstrap': bootstrap,\n",
    "    'random_state': random_state,\n",
    "    'class_weight': class_weight\n",
    "    }\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_grid = GridSearchCV(estimator=rf_model, param_grid=search_grid, \n",
    "    cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "rf_grid.fit(X, y)\n",
    "\n",
    "model_stat(rf_model, X, y)\n",
    "print(rf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.5s remaining:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9429114531596852\n",
      "0.9182819064174996\n",
      "0.9709120015574658\n",
      "{'C': 1.05, 'class_weight': 'balanced', 'gamma': 0.05, 'kernel': 'rbf', 'max_iter': -1, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# tune SVM\n",
    "search_grid = {\n",
    "    'C': [1.05], \n",
    "    'kernel': ['rbf'],\n",
    "    'gamma': [0.05],\n",
    "    'class_weight': ['balanced'],\n",
    "    'max_iter': [-1],\n",
    "    'random_state': [0]\n",
    "}\n",
    "svm_model = SVC()\n",
    "svm_grid = GridSearchCV(estimator=svm_model, param_grid=search_grid, \n",
    "    cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "svm_grid.fit(X, y)\n",
    "model_stat(svm_model, X, y)\n",
    "print(svm_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.4s remaining:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    2.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8939239960207626\n",
      "0.8433869332174417\n",
      "0.9704244061231451\n",
      "{'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 85, 'min_samples_split': 2, 'n_estimators': 150, 'random_state': 0, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# tune GBDT\n",
    "search_grid = {\n",
    "    'n_estimators': [150],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [85],\n",
    "    'max_depth': [8],\n",
    "    'max_features':['sqrt'],\n",
    "    'subsample': [0.8],\n",
    "    'random_state': [0]\n",
    "}\n",
    "gbdt_model = GradientBoostingClassifier()\n",
    "gbdt_grid = GridSearchCV(estimator= gbdt_model, param_grid=search_grid, \n",
    "    cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "gbdt_grid.fit(X, y)\n",
    "model_stat(gbdt_model, X, y)\n",
    "print(gbdt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.2s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.901383218681918\n",
      "0.8416050992322178\n",
      "0.9818254406583009\n",
      "{'criterion': 'entropy', 'max_depth': 8, 'max_features': 'sqrt', 'min_samples_leaf': 50, 'min_samples_split': 281, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "# tune DT (TBI)\n",
    "search_grid = {\n",
    "    'criterion': ['entropy'],\n",
    "    'min_samples_split': [281],\n",
    "    'min_samples_leaf': [50],\n",
    "    'max_depth': [8],\n",
    "    'max_features':['sqrt'],\n",
    "    'random_state': [0]\n",
    "}\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_grid = GridSearchCV(estimator= dt_model, param_grid=search_grid, \n",
    "    cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "dt_grid.fit(X, y)\n",
    "model_stat(dt_model, X, y)\n",
    "print(dt_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    2.5s remaining:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    3.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9003245400338121\n",
      "0.8502824858757062\n",
      "0.9714280041718005\n",
      "{'algorithm': 'RGF_Sib', 'loss': 'Log', 'max_leaf': 1000, 'test_interval': 100}\n"
     ]
    }
   ],
   "source": [
    "# tune RGF\n",
    "search_grid = {\n",
    "    'max_leaf': [1000],\n",
    "    'algorithm': ['RGF_Sib'],\n",
    "    'test_interval': [100],\n",
    "    'loss': ['Log']\n",
    "}\n",
    "# loss: You can select \"LS\", \"Log\", \"Expo\" or \"Abs\".\n",
    "rgf_model = RGFClassifier()\n",
    "rgf_grid = GridSearchCV(estimator= rgf_model, param_grid=search_grid, \n",
    "    cv=5, scoring='f1', n_jobs=-1, verbose=2)\n",
    "rgf_grid.fit(X, y)\n",
    "model_stat(rgf_model, X, y)\n",
    "# print(rgf_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lambda x: x+5)(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
